{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c4354a",
   "metadata": {},
   "source": [
    "Code for Byte_pair_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22029999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513018b",
   "metadata": {},
   "source": [
    "Instantiate BPE tokenizer from tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304eded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d09c1d",
   "metadata": {},
   "source": [
    "Testing the instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709eef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text,allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a914cab",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "1) End of text token is assigned to a relatively large token ID.\n",
    "\n",
    "2) BPE tokenizer can handle any unknown word as the algorithm underlying BPE tokens breaks down words that aren't in its predefined vocabulary into smaller sutiable words.\n",
    "\n",
    "3) This enables BPE algorithm to handle out-of vocabulary words.\n",
    "\n",
    "4) If it encounters unkown words it will represent them with a sequence of known words and characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf56d0",
   "metadata": {},
   "source": [
    "Code to understand how BPE tokenizer encodes unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d565392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
