{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d25f809d",
   "metadata": {},
   "source": [
    "Peforming tokenization in python using an example text file\n",
    "\n",
    "Total number of characters present in this text file are 20479"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ebbab",
   "metadata": {},
   "source": [
    "Libraires used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771f6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb73a9f",
   "metadata": {},
   "source": [
    "Step1:Create Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a55fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"../Required_files/the-verdict.txt\",'r',encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters\",len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5cb18",
   "metadata": {},
   "source": [
    "while tokenization  whitespaces are removed then it will reduce the size of memory but sometimes inclusion of whitespaces can be meaningful for example python code is sensitive to identations and spacing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a61c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
      "4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([.,:;?_!\"()\\']|--|\\s)',raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0535c5",
   "metadata": {},
   "source": [
    "Step2: Creating token Id's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcdfb9",
   "metadata": {},
   "source": [
    "For creating tokenid's only unique words are taken in aplabetical order.\n",
    "\n",
    "This is the reason vocab size taken for conversion to token id's is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9b1491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0c879",
   "metadata": {},
   "source": [
    "Python dictionary relating tokens and token id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3e9c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3752b7",
   "metadata": {},
   "source": [
    "Step3: Creating Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5fa028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuation\n",
    "        text = re.sub(r'\\s+([.,:;?_!\"()\\'])', r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54c158",
   "metadata": {},
   "source": [
    "Test for tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e4c67",
   "metadata": {},
   "source": [
    "For encode method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fbaaec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a9507a",
   "metadata": {},
   "source": [
    "For decode method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3488a03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf2b75",
   "metadata": {},
   "source": [
    "Adding unknown words token and end of sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "002843db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {tokenizer:integer for integer,tokenizer in enumerate(all_words)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4df23",
   "metadata": {},
   "source": [
    "Tokenizer class including unkown word token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f2862cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                       else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuation\n",
    "        text = re.sub(r'\\s+([.,:;?_!\"()\\'])', r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2cfe3",
   "metadata": {},
   "source": [
    "Test for SimpleTokenizerV2 task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3fa8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1,text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72f70e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "954c08b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae241c3",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Generally while adding two diffrent text from two diffrent sources \"end of text\" will be inserted between them.\n",
    "\n",
    "After end of text token a space has to be provided otherwise end of token will get combined with next sentence without space.\n",
    "\n",
    "So, when text is splitted token will be created as [<endoftext\\>]nextword , as token will be crated in this way it will be mapped to unk token id.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
