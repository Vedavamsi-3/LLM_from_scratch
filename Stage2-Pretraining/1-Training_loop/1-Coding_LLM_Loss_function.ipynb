{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14994a74",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e664ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from coding_gpt2_to_predict_next_token import GPTModel,generate_text_simple\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b24df7",
   "metadata": {},
   "source": [
    "Use GPT to genrate text\n",
    "\n",
    "hTe context length (context_length) considered is only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25df6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b04c3e",
   "metadata": {},
   "source": [
    "Generate next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da62590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 34245]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 34245,  5139]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434, 17853]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434, 17853,\n",
      "         33246]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434, 17853,\n",
      "         33246, 10306]])\n",
      "tensor([[ 6109,  3626,  6100,   345, 34245,  5139,  2492, 25405, 17434, 17853,\n",
      "         33246, 10306, 14856]])\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids (text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model =model,\n",
    "    idx =   text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens =10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747642e1",
   "metadata": {},
   "source": [
    "Calculating text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a1a56",
   "metadata": {},
   "source": [
    "Suppose we have an inputs tensor containing the token IDs for 2 training examples (rows)\n",
    "\n",
    "Corresponding to the inputs, the targets contain the desired token IDs that we want the model to generate\n",
    "\n",
    "Notice that the targets are the inputs shifted by 1 position, as explained in chapter 2 when we implemented the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c180d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fedcd147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e62ad263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[10681],\n",
      "         [16031],\n",
      "         [42826]],\n",
      "\n",
      "        [[35456],\n",
      "         [ 7822],\n",
      "         [ 2086]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f6441",
   "metadata": {},
   "source": [
    "If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens.\n",
    "\n",
    "That's because the model wasn't trained yet.\n",
    "\n",
    "To train the model, we need to know how far it is away from the correct predictions (targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47bddf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  unemployment savesNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f41f9",
   "metadata": {},
   "source": [
    "Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f98ca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([4.1759e-05, 1.9837e-05, 1.1408e-05])\n",
      "Text 2: tensor([1.1142e-05, 4.1978e-05, 5.0496e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7980d47",
   "metadata": {},
   "source": [
    "We want to maximize all these values, bringing them close to a probability of 1.\n",
    "    \n",
    "In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9bc87f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.0836, -10.8280, -11.3812, -11.4048, -10.0784, -12.1962])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28317e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.9953)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27ce84",
   "metadata": {},
   "source": [
    "The goal is to make this average log probability as large as possible by optimizing the model weights.\n",
    "\n",
    "Due to the log, the largest possible value is 0, and we are currently far away from 0.\n",
    "\n",
    "In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0.\n",
    "\n",
    "The value negative of -10.7722, i.e., 10.7722, is also called cross-entropy loss in deep learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a039d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9953)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c213a",
   "metadata": {},
   "source": [
    "PyTorch already implements a cross_entropy function that carries out the previous steps\n",
    "\n",
    "Before we apply the cross_entropy function, let's check the shape of the logits and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c591c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9dbc5d",
   "metadata": {},
   "source": [
    "For the cross_entropy function in PyTorch, we want to flatten these tensors by combining them over the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d41d2840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5d7662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9953)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8213ab47",
   "metadata": {},
   "source": [
    "Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac37438d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(59596.3086)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c7c7a",
   "metadata": {},
   "source": [
    "The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 48,725 words or tokens).\n",
    "\n",
    "In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset.\n",
    "    \n",
    "Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
