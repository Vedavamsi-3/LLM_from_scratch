{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f23355",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9400ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6306b20",
   "metadata": {},
   "source": [
    "Confriguration considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3a2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50527,\n",
    "    \"context_length\" : 768,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"n_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6a02c",
   "metadata": {},
   "source": [
    "Layer normalization and Feed forward neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779b7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim = True)\n",
    "        variance = x.var(dim = -1, keepdim =True)\n",
    "        norm_x = (x-mean) / torch.sqrt(variance + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi)) * \n",
    "        (x + 0.0044715*torch.pow(x,3))))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772e774",
   "metadata": {},
   "source": [
    "Multi-Head attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca923240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,d_in,d_out,context_length,dropout=0.5,num_heads=2 ,qkvbias = False):\n",
    "\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in,d_out,qkvbias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,qkvbias)\n",
    "        self.W_value = nn.Linear(d_in,d_out,qkvbias)\n",
    "        self.out_proj = nn.Linear(d_out,d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(context_length, context_length),diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, num_tokens , d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)  # (b, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1,2)  # (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.transpose(1,2)  # (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] ==0\n",
    "\n",
    "        attn_scores = attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim =-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector = (attn_weights @ values ).transpose(1,2) #Shape: (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "        context_vector = context_vector.contiguous().view(b,num_tokens,self.d_out)\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d96ca",
   "metadata": {},
   "source": [
    "Transformer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637b8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in = cfg['emb_dim'],\n",
    "            d_out = cfg['emb_dim'],\n",
    "            context_length = cfg['context_length'],\n",
    "            num_heads = cfg['n_heads'],\n",
    "            dropout = cfg['drop_rate'],\n",
    "            qkvbias = cfg['qkv_bias'])\n",
    "\n",
    "        self.ffn = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.dropout_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Shortcut connection for feedforward block\n",
    "        shortcut =x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout_shortcut(x)\n",
    "        x = x + shortcut    \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682dd02",
   "metadata": {},
   "source": [
    "Full_GPT_archetecture_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ce0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pok_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.outhead = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias = False)\n",
    "\n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_length = in_idx.shape\n",
    "        tok_embeds  =self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pok_emb(torch.arange(seq_length, device = in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.outhead(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ebd281",
   "metadata": {},
   "source": [
    "Generate next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456284cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model,idx,max_new_tokens,context_size):\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        print(idx_cond)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:,-1,:]\n",
    "\n",
    "        probas = torch.softmax(logits,dim =-1) # (batch, vocab_size)\n",
    "\n",
    "        idx_next = torch.argmax(probas, dim =-1, keepdim = True) # (batch, 1)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim =1) # (batch, n_tokens+1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf428967",
   "metadata": {},
   "source": [
    "The softmax function is monotonic, meaning it preserves the order of its inputs when transformed into outputs. \n",
    "\n",
    "So, in practice, the softmax step isredundant since the position with the highest score in the softmax output tensor is the\n",
    "same position in the logit tensor. \n",
    "\n",
    "In other words, we could apply the torch.argmax function to the logits tensor directly and get identical results. \n",
    "\n",
    "\n",
    "But implementing softmax will be useful when applying addtional additional sampling techniques w in the model where we modify the softmax outputs such that the model doesn't always select the most likely token, which introduces variability andcreativity in the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8d721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor: tensor([[15496,    11,   314,   716]])\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37016d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,    11,   314,   716]])\n",
      "tensor([[15496,    11,   314,   716, 45345]])\n",
      "tensor([[15496,    11,   314,   716, 45345, 38383]])\n",
      "tensor([[15496,    11,   314,   716, 45345, 38383,  4446]])\n",
      "tensor([[15496,    11,   314,   716, 45345, 38383,  4446, 18989]])\n",
      "tensor([[15496,    11,   314,   716, 45345, 38383,  4446, 18989, 24224]])\n",
      "Ouput: tensor([[15496,    11,   314,   716, 45345, 38383,  4446, 18989, 24224,  3023]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "out  = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens =6,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Ouput:\",out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
